
# 序文
この本では、さまざまな現実に向き合ったソフトウェアのアーキテクチャ、デザイン、作り方についてみていく。

## 対象読者
アーキテクト、デザイナー、開発者。

## この本の構成
4つのパートに分かれており、各パートはケーススタディから始まる。
4つのパートとは、

* 安定性について
* 本番環境の設計について
* デプロイについて
* 運用しやすいシステムの構築について

である。

## ケーススタディについて
具体的な社名等は伏せているが、実際に筆者が経験したことについて書いている。

# 1章 本番環境で稼働する
仕様を実装し、直ちに本番環境にデプロイできるわけではない。
また、テストを通したとしても想定外のことが必ず起きる。

## 正しく標準を合わせよ
ローコストで高品質なものを作るべき。？？

## チャレンジのスコープ
ソフトウェアを早く、安く作ったり、操作しやすさを求めたり、などチャレンジすることが多くなるほどアーキテクチャや設計は継続的に改善する必要が出てくる。

## 百万ドルがあちこちに
開発時の工数のみを気にして、運用のことをないがしろにするのはナンセンス。なぜなら、開発は1度きりだが、運用は以後ずっと続くからだ。
本書では、技術的な視点と経営的な視点が大事なものとなってくる。

## フォースを使え
設計によって、その他に与える影響は大きく異なる。本書ではその例を見せていく。

## 現実的なアーキテクチャ
抽象に逃げず、現実を見て設計すべし。

## 要するに
本書では最初のリリースから、それ以降のシステムの拡張について扱う。

# パート1 安定性を作る

# 2章 ケーススタディ：エアラインを止めた例外
小さなエラーが雪だるま式に膨れ上がり、航空会社に大損失を与えた例をあげる。

エラーの発端はcore facilities（CF）に関わるデータベースの計画フェイルオーバーだった。
エラーが発生したとき、この航空会社はSOAへの移行過渡期であり、CFは第一世代のものであった。
構成としては以下の図のようで、単一障害点のない高可用性アーキテクチャであった。

![構成](リリースイット_2章.bmp)

## 窓を変える
計画フェイルオーバーでは、アプリからの接続先DBを1から2に変えて、2につながっている間に、1をアップデートし、それが終わると1に接続を戻し、2も同様にアップデートした。
その時の作業はうまくいき、問題は何も起こらなかった。

## 停止
AM2:30に自動チェックイン機が停止した。数分後にIVRサーバーも停止した。
停止した両システムは、以下の依存関係からわかるように、どちらもCFに依存している。

![依存関係](リリースイット_2章_2.bmp)

結局、CFと自動チェックイン機を再起動することで、停止から3時間たって復旧した。

## 結果
たった3時間の停止に見えるがその影響は大きい。

## 死後
障害発生から八時間後のAM10:30に筆者への調査依頼が届く。
障害発生後に解析を行うのは、手掛かりがログや障害時に集めたデータしかなく、難しい。

## 手がかりを探す
これまでの知見をもとにDBのクラスタ構成にバグがないか探すも、バグはなし。
次にアプリケーションサーバのスレッドダンプを見ることにした。
スレッドダンプを読めれば、多くのことが推論できる。

* アプリがどのようなサードパーティライブラリを使用しているか？
* どのような種類のスレッドプールをもっているか？
* 各スレッドプールには何スレッドあるか？
* アプリケーションはどのようなバックグラウンド処理をしているか？
* アプリケーションはどのようなプロトコルを使用しているか？

自動チェックイン機サーバーのスレッドダンプは予想通りのものであった。
各チェックイン機からのリクエストを処理するスレッドが40あるうち、全部がJavaのソケットライブラリの内部のnativeメソッドである、SocketInputStream.socketRead0でブロックされていた。
チェックイン機サーバーの全40スレッドはFlightSearch.lookupByCityというメソッドから呼ばれていることが分かったが、スタック上のそのメソッドの少し上にRMIとEJBのメソッドへの参照があった。

RMIはEJBにおけるリモートでの処理を可能にする。RMIを用いると、マシン間の通信が隠ぺいされるものの、呼び出しのタイムアウトがないため危険である。

## 決定的証拠
残るはCFにおいて何が起こったかを確かめることである。
 
スレッドダンプを調査したところ、全アプリの全スレッドが、リソースプールからコネクションを引っ張ってくるところでブロックされていたことが分かった。
 
状況証拠に過ぎないが、障害前にDBのフェイルオーバーがあったことを考えると、正しい推論のようだ。
 
次に、コードをみようとしたが、障害発生後特有の、誰かに責任を押し付けようという空気感のため、見せてもらえなかった。
 
そのため、本番環境からバイナリを取得し、逆コンパイルしてコードを見たところ、致命的な欠陥を抱えていることが分かった。
それが以下。

```
try  {
  conn = connectionPool.getConnection();
  stmt = conn.createStatement();
   // Do the lookup logic 
   // return a list of results
} finally {
  if  (stmt != null) {
    stmt.close();
  }
  if (conn !=null) {
    conn.close();
  }
}
```

java.sql.Statement.closeメソッドはデータベースのフェイルオーバー後などに実行するとエラーが発生することがある。
 
フェイルオーバー後はTCPの接続が引き継がれず、OSとネットワークドライバがTCPが切断されたと判断したときに、ソケットはIOExceptionをスローする。
 
JDBCのコネクションはTCP切断が切れた後もステートメントを受け入れ続け、ステートメントを実行した際にSQLExceptionがスローされ、Statementのcloseでも例外が発生する。
 
ここでの教訓は、JDBCの使用上、java.sql.Statement.closeはSQLExceptionを投げうるので、適切に扱わねばならないということだ。
 
上記のコードでは、Statementのcloseで例外が起こると、コネクションのcloseはされないので、結果としてブロックされるようになったのだった。
 
大規模障害は一つのSQLExceptionを適切に処理しないことにより起こったと言える。

## ほんの少しの予防？
どうすればこれを予防することができたか？見るべきところが分かれば、テストを実施することは容易である。
 
究極的には、バグを予見しきることは不可能である。
 
最悪なのは、1つのシステムのバグが伝播していくことである。例でみたようなバグの伝播が起きないような設計手法を以後みていく。

# 3章 システムを安定稼働させよ
現実世界で起こることはテストでの予想の範疇を超えてくる。
 
エンタープライズソフトウェアは、エラーは必ず起きるものと前提を置き、他システムとの関係は疎であることを保つようにする。
 
2章の例では、互いに疎な関係を保てていなかった。
 
低い安定性は重大な利益損失につながる。
 
また、安定性は企業の評判にも響いてくる。
 
驚くことに、高度な安定性を保った設計と、そうでない設計の実装にかかるコストはたいてい同じくらいである。

## 安定性を定義する
安定性について話すために、いくつかの用語を定義する。
transactionは、データベースのトランザクションではなく、eコマースにおける購入の一連の流れのようなものを指す。
mixed workloadは、transactionの組み合わせを指す。
 
systemは、ユーザーがtransactionを実行するために必要な諸々を指す。
 
堅牢なsystemは一時的なimpulseや、永続的なstressが特定期間かかっても、transactionを処理し続ける。
 
impulseはsystemに対する急激な刺激のようなものを指す。
対して、stressはsystemに対して特定期間かかり続ける負荷を指す。
 
impulseが原因で起こる問題の例としては、著名人があなたのwebサイトについて言及して、急激にアクセス数が増加するようなことがあげられる。
 
stressが原因で起こる問題の例としては、クレジットカードの決済処理機能を使用し続けて、キャパシティの問題で処理が遅くなるようなことがあげられる。
 
息の長いsystemはtransitionを長時間処理し続ける。長時間の定義としては、コードデプロイの間隔を用いるとよい。（？ぴんと来ない）
 
## 寿命を延ばす
システムの寿命を脅かすものでよくあるのは、メモリリークとデータの増大であり、これらはテストでは気づきにくい。
 
マーフィーの法則によると、テストできない事柄は必ず起こると考えたほうが良い。
 
困ったことに、本番環境で稼働させる以上の時間をかけて、開発環境でアプリケーションを稼働させることは普通できない。
 
この種のバグは負荷テストでも検知することはできない。なぜなら、負荷テストではそんなに長期ではない一定の期間しか負荷をかけ続けないからだ。
 
どのようにすればこの種のバグを発見できるだろうか？
常にリクエストを送り続けるようなテストを実施すれば発見できる。（JMeter等つかってリクエストを送り続ける）
 
経済的な理由で完璧な環境が得られずとも、コアな機能では上記のようなテストを実施したほうが良い。
 
それもダメならぶっつけ本番になってしまうが、平穏に過ごしたいなら、ちゃんと本番前にテストしたほうがいい。


## 故障モード
バグのトリガー、システムの残りの部分への広がり方、その結果、全部ひっくるめて故障モードという。
 
故障の内容を検知すれば、影響をシステム内にとどめておけるような安全な故障モードを設計できる。
 
故障モードを定義しておかないと、急に予期せぬ事象に対応する羽目になる。
 
## 亀裂の伝播を止める
2章の例でどのように故障モードを設計していくか見ていく。
この例では、SQLExceptionの不適切なハンドリングが問題源であったが、広がりを防ぐ方法はいくつかあった。
それらを様々なレイヤの観点からみていく。
 
コネクションプールに余りがない場合にはリクエストしてきたスレッドをブロックする設定になっていたが、新しいコネクションを作成したり、一定時間だけブロックする設定にしていれば故障の広がりは防げた。
 
CFはRMIで呼ばれるEJBを用意していたが、RMIの呼び出しはデフォルトではタイムアウトすることがない。
つまり、呼び出し元は、最初の20の呼び出しでSQLExceptionをラップした例外を受け取った後、ブロッキングが始まったということである。
 
RMIのソケットにタイムアウトを設定したり、そもそもEJBではなく、HTTPベースでの連携にしてタイムアウト設定をしておけば、CF外に障害が広がることはなかった。
 
より大きなスケールでの対策として、CFサーバを別のサービスグループに区分けしておく方法がある。これによって問題を1つのサービスグループ内に閉じ込めておくことができる。（今回の例では全サービスグループで同じ問題が起きるが）
**サービスグループってなんだ？**
 
さらに大きなアーキテクチャ上の話をすると、CFとのやりとりはメッセージキューを使うこともできた。
これにより、システム同士の結合を疎に保つことができ、障害の伝播を抑えることができる。
 
## 故障の連鎖
全てのシステム停止には上記で見たような故障の連鎖がある。
個々の事象を切り取ってみると、とても起こりそうにないように見えるが、これらは密接に関連しているので、他の故障に合わせて自身の故障確立も上がってしまう。
 
イベントの連鎖について、用語を簡単にまとめる。

* Fault（障害）：ソフトウェア内に不正な状況を作り出してしまう状態
* Error（エラー）：目に見えて不正な挙動をする状態
* Failure（故障）：システムが反応しない状態

 
FaultがErrorとなり、ErrorがFailureを引き起こす。このようにしてダメージが広がっていく。
 
複雑なシステムには様々なダメージの広がり方や、エラーとなるきっかけがある。
 
また、蜜結合したシステムはダメージの広がり障害を加速させる。
 
起きうる故障に備える一つの方法としては、全てのメソッド呼び出し、I/O、リソース使用、呼び出し予想結果をみていき、これらがうまくいかなかった場合にどうなるか自問自答し、様々なimpulse、stressについても考慮する方法がある。
 
上記のことはほんの一部であり、ブルートフォース的なアプローチですべてを洗い出すことは非現実的である。
 
障害の扱いに関しては、コミュニティの中でも2つの陣営がある。
1つはフォールトトレランスなシステムにすべきという陣営で、もう1つはフォールトトレランスにしても無意味であるという陣営である。
 
しかし、両陣営でも合意していることが2つある。

* Faultは起きるということ
* FaultをErrorにしてはならないということ

## 締めくくり
故障にはパターンがあるので、それを4章で紹介する。
 
5章ではアンチパターンを克服するための設計・アーキテクチャのパターンを紹介する。
そこでは障害のきっかけを防ぐ方法ではなく、ダメージを一部にとどめておく方法の説明を行う。


# 4章 安定性アンチパターン
本章ではシステムを止めてしまうアンチパターンを見ていき、事象でアンチパターンへの対抗策を見ていく。
 
## インテグレーションポイント
現代のウェブサイトはHTML、フロントエンドアプリ、APIなどを結合させたものとなっており、その形式はバタフライ型と蜘蛛の巣型に二分される。
バタフライ型はたくさんのコネクション、フィードを持つ中央システムが仲介する。
このようなシステムをモノリスと呼ぶ。

![バタフライ型](リリースイット_4章_1.bmp)

 
もう一方は蜘蛛の巣のようなスタイルをとる。
サービスをティアで分けると次の1つ目のような構造になるが、わけない場合は2つ目のようなカオスな構造になる。


![蜘蛛の巣型-1](リリースイット_4章_2.bmp)

![蜘蛛の巣型-2](リリースイット_4章_3.bmp)

バタフライ型は2Nのコネクションがあり、蜘蛛の巣型は最大で2のN乗のコネクションがあり得る。
全てのコネクションがインテグレーションポイントであり、システムを脅かす。
 
インテグレーションポイントはシステムにとってNo.1キラーである。
これ以降はインテグレーションポイントがどのように悪さをするかと、どのように対処すればよいかを見ていく。

### ソケットベースプロトコル
高レイヤにおいては名前付きパイプ、共有メモリIPCを除けばだいたいのインテグレーションプロトコルはソケットの仕組みを利用している。
 
最もシンプルな故障モードはリモートシステムがコネクションを拒絶した場合であり、これは扱いがしやすい。
 
ただし、1つ気をつけねばならないのは、接続ができないとわかるまでに長時間かかる場合だ。

ソケットがブロックをし続けないようにタイムアウトの設定をしておく必要がある。
 
ネットワーク故障には2種あり、1つは早く反応が返ってくるもので、もう1つは反応がなかなか返ってこないものだ。
反応が返ってこないものは、場合によってはサーバをダウンさせてしまう。

### 午前5時の問題
毎朝5時に障害が起こるサイトがあった。
そのサイトは30のサーバインスタンスで運営されており、5分間の間で全インスタンスがハングしていた。
再起動によって常にその問題は解決されていたので、朝5時に何かが起こったと言える。
 
リリース後3日目に障害が起こり、スレッドダンプをみると、スレッドはオラクルのJDBCライブラリのOCIコールでブロックされていた。
ブロックされていたスレッドを殺し、再度DBとの同期を図ると、低レイヤでのread、writeがおこっているように見えた。
 
次に、tcpdumpとWiresharkを使った。
不思議なことに、アプリケーションサーバからパケットを送っているのに、データベースからの返答がなかった。（DBの状態は正常）
 
同じ障害が翌朝にも起きた。
その時、ファイアウォールよりDB側にはパケットがなにも流れていなかったことから仮説を立てた。
アプリケーションサーバのリソースプールのクラスをデコンパイルしたところ、立てた仮設に確信をもった。
 
TCPのコネクションはエンドポイントのメモリにその情報があれば、1つもパケットのやり取りがなくとも数日間コネクションを保てる。
 
